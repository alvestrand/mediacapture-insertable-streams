<pre class='metadata'>
Title: MediaStreamTrack Insertable Media Processing using Streams
Shortname: mediacapture-insertable-streams
Level: None
Status: UD
Group: webrtc
Repository: w3c/mediacapture-insertable-streams
URL: https://w3c.github.io/mediacapture-insertable-streams/
Editor: Harald Alvestrand, Google https://google.com, hta@google.com
Editor: Guido Urdaneta, Google https://google.com, guidou@google.com
Abstract: This API defines an API surface for manipulating the bits on
Abstract: {{MediaStreamTrack}}s carrying raw data.
Abstract: NOT AN ADOPTED WORKING GROUP DOCUMENT.
Markup Shorthands: css no, markdown yes
</pre>
<pre class=anchors>
url: https://wicg.github.io/web-codecs/#videoframe; text: VideoFrame; type: interface; spec: WEBCODECS
url: https://wicg.github.io/web-codecs/#audiodata; text: AudioData; type: interface; spec: WEBCODECS
url: https://www.w3.org/TR/mediacapture-streams/#mediastreamtrack; text: MediaStreamTrack; type: interface; spec: MEDIACAPTURE-STREAMS
url: https://www.w3.org/TR/mediacapture-streams/#dom-constrainulong; text: ConstrainULong; type: typedef; spec: MEDIACAPTURE-STREAMS
url: https://www.w3.org/TR/mediacapture-streams/#dom-constraindouble; text: ConstrainDouble; type: typedef; spec: MEDIACAPTURE-STREAMS
url: https://www.w3.org/TR/mediacapture-streams/#dom-constraindomstring; text: ConstrainDOMString; type: typedef; spec: MEDIACAPTURE-STREAMS
url: https://www.w3.org/TR/mediacapture-streams/#dom-videoresizemodeenum; text: VideoResizeModeEnum; type: enum; spec: MEDIACAPTURE-STREAMS
url: https://w3c.github.io/mediacapture-main/#idl-def-VideoResizeModeEnum.user; text: none; for: VideoResizeModeEnum; type: enum; spec: MEDIACAPTURE-STREAMS
url: https://w3c.github.io/mediacapture-main/#idl-def-VideoResizeModeEnum.right; text: crop-and-scale; for: VideoResizeModeEnum; type: enum; spec: MEDIACAPTURE-STREAMS
url: https://infra.spec.whatwg.org/#queues; text: Queue; type: typedef; spec: INFRA
url: https://infra.spec.whatwg.org/#queue-enqueue; text: enqueue; for: Queue; type: typedef; spec: INFRA
url: https://infra.spec.whatwg.org/#queue-dequeue; text: dequeue; for: Queue; type: typedef; spec: INFRA
url: https://infra.spec.whatwg.org/#list-is-empty; text: empty; for: Queue; type: typedef; spec: INFRA
url: https://infra.spec.whatwg.org/#booleans; text: Boolean; type: typedef; spec: INFRA
url: https://www.w3.org/TR/mediacapture-streams/#source-stopped; text: StopSource; type: typedef; spec: MEDIACAPTURE-STREAM
url: https://www.w3.org/TR/mediacapture-streams/#track-ended; text: TrackEnded; type: typedef; spec: MEDIACAPTURE-STREAMS
url: https://streams.spec.whatwg.org/#readable-stream-default-controller-close; text: ReadableStreamDefaultControllerClose; type: typedef; spec: STREAMS
url: https://streams.spec.whatwg.org/#readablestream-controller; text: ReadableStreamControllerSlot; type: typedef; spec: STREAMS
url: https://infra.spec.whatwg.org/#list-empty; text: Empty; for: List; type: typedef; spec: INFRA
url: https://infra.spec.whatwg.org/#list-remove; text: remove; for: List; type: typedef; spec: INFRA
</pre>
<pre class=biblio>
{
  "WEBCODECS": {
     "href":
     "https://wicg.github.io/web-codecs/",
     "title": "WebCodecs"
   },
  "MEDIACAPTURE-SCREEN-SHARE": {
    "href": "https://w3c.github.io/mediacapture-screen-share/",
    "title": "Screen Capture"
  },
  "MEDIACAPTURE-STREAMS": {
    "href": "https://www.w3.org/TR/mediacapture-streams/",
    "title": "Media Capture and Streams"
  },
  "INFRA": {
    "href": "https://https://infra.spec.whatwg.org",
    "title": "Infra"
  },
  "STREAMS": {
    "href": "https://streams.spec.whatwg.org",
    "title": "Streams"
  }
}
</pre>
<pre class=link-defaults>
spec:streams; type:interface; text:WritableStream
</pre>

# Introduction # {#introduction}

The [[WEBRTC-NV-USE-CASES]] document describes several functions that
can only be achieved by access to media (requirements N20-N22),
including, but not limited to:
* Funny Hats
* Machine Learning
* Virtual Reality Gaming

These use cases further require that processing can be done in worker
threads (requirement N23-N24).

This specification gives an interface inspired by [[WEBCODECS]] to
provide access to such functionality.

This specification provides access to raw media,
which is the output of a media source such as a camera, microphone, screen capture,
or the decoder part of a codec and the input to the
decoder part of a codec. The processed media can be consumed by any destination
that can take a MediaStreamTrack, including HTML &lt;video&gt; and &lt;audio&gt; tags,
RTCPeerConnection, canvas or MediaRecorder.

# Terminology # {#terminology}

# Specification # {#specification}

This specification shows the IDL extensions for [[MEDIACAPTURE-STREAMS]].
It defines some new objects that inherit the {{MediaStreamTrack}} interface, and
can be constructed from a {{MediaStreamTrack}}.

The API consists of two elements. One is a track sink that is
capable of exposing the unencoded media frames from the track to a ReadableStream.
The other one is the inverse of that: it provides a track source that takes
media frames as input.

<!-- ## Extension operation ## {#operation} -->

## MediaStreamTrackProcessor ## {#track-processor}

A {{MediaStreamTrackProcessor}} allows the creation of a
{{ReadableStream}} that can expose the media flowing through
a given {{MediaStreamTrack}}. If the {{MediaStreamTrack}} is a video track,
the chunks exposed by the stream will be {{VideoFrame}} objects;
if the track is an audio track, the chunks will be {{AudioData}} objects.
This makes {{MediaStreamTrackProcessor}} effectively a sink in the
<a href="https://www.w3.org/TR/mediacapture-streams/#the-model-sources-sinks-constraints-and-settings">
MediaStream model</a>.

A {{MediaStreamTrackProcessor}} internally contains a circular queue
that allows buffering incoming media frames delivered by the track it
is connected to. This buffering allows the {{MediaStreamTrackProcessor}}
to temporarily hold frames waiting to be read from its associated {{ReadableStream}}.
The application can influence the maximum size of the queue via a parameter
provided in the {{MediaStreamTrackProcessor}} constructor. However, the
maximum size of the queue is decided by the UA and can change dynamically,
but it will not exceed the size requested by the application.
If the application does not provide a maximum size parameter, the UA is free
to decide the maximum size of the queue.

When a new frame arrives to the
{{MediaStreamTrackProcessor}}, if the queue has reached its maximum size,
the oldest frame will be removed from the queue, and the new frame will be
added to the queue. This means that for the particular case of a queue
with a maximum size of 1, if there is a queued frame, it will aways be
the most recent one.

The UA is also free to remove any frames from the queue at any time. The UA
may remove frames in order to save resources or to improve performance in
specific situations. In all cases, frames that are not dropped
must be made available to the {{ReadableStream}} in the order in which
they arrive to the {{MediaStreamTrackProcessor}}.

A {{MediaStreamTrackProcessor}} makes frames available to its
associated {{ReadableStream}} only when a read request has been issued on
the stream. The idea is to avoid the stream's internal buffering, which
does not give the UA enough flexibility to choose the buffering policy.

### Interface definition ### {#track-processor-interface}

<pre class="idl">
interface MediaStreamTrackProcessor {
    constructor(MediaStreamTrackProcessorInit init);
    attribute ReadableStream readable;
};

dictionary MediaStreamTrackProcessorInit {
  required MediaStreamTrack track;
  [EnforceRange] unsigned short maxBufferSize;
};
</pre>

### Internal slots ### {#internal-slots-processor}
<dl>
<dt><dfn for=MediaStreamTrackProcessor>`[[track]]`</dfn></dt>
<dd>Track whose raw data is to be exposed by the {{MediaStreamTrackProcessor}}.</dd>
<dt><dfn for=MediaStreamTrackProcessor>`[[maxBufferSize]]`</dfn></dt>
<dd>The maximum number of media frames to be buffered by the {{MediaStreamTrackProcessor}}
as specified by the application. It may have no value if the application does
not provide it. Its minimum valid value is 1.</dd>
<dt><dfn for=MediaStreamTrackProcessor>`[[queue]]`</dfn></dt>
<dd>A {{Queue|queue}} used to buffer media frames not yet read by the application</dd>
<dt><dfn for=MediaStreamTrackProcessor>`[[numPendingReads]]`</dfn></dt>
<dd>An integer whose value represents the number of read requests issued by the
application that have not yet been handled.
</dd>
<dt><dfn for=MediaStreamTrackProcessor>`[[isClosed]]`</dfn></dt>
<dd>An boolean whose value indicates if the {{MediaStreamTrackProcessor}} is closed.
</dd>
</dl>

### Constructor ### {#constructor-processor}
<dfn constructor for=MediaStreamTrackProcessor title="MediaStreamTrackProcessor(init)">
  MediaStreamTrackProcessor(|init|)
</dfn>
1. If |init|.{{MediaStreamTrackProcessorInit/track}} is not a valid {{MediaStreamTrack}},
    throw a {{TypeError}}.
2. Let |processor| be a new {{MediaStreamTrackProcessor}} object.
3. Assign |init|.{{MediaStreamTrackProcessorInit/track}} to |processor|.`[[track]]`.
4. If |init|.{{MediaStreamTrackProcessorInit/maxBufferSize}} has a integer value greater than or equal to 1, assign it to |processor|.`[[maxBufferSize]]`.
6. Set the `[[queue]]` internal slot of |processor| to an empty {{Queue}}.
7. Set |processor|.`[[numPendingReads]]` to 0.
8. Set |processor|.`[[isClosed]]` to false.
9. Return |processor|.

### Attributes ### {#attributes-processor}
<dl>
<dt><dfn for=MediaStreamTrackProcessor>readable</dfn></dt>
<dd>Allows reading the frames delivered by the {{MediaStreamTrack}} stored
in the `[[track]]` internal slot. This attribute is created the first time it is invoked
according to the following steps:
1. Initialize [=this=].{{MediaStreamTrackProcessor/readable}} to be a new {{ReadableStream}}.
2. <a dfn for="ReadableStream">Set up</a> [=this=].{{MediaStreamTrackProcessor/readable}} with its [=ReadableStream/set up/pullAlgorithm=] set to [=processorPull=] with [=this=] as parameter, [=ReadableStream/set up/cancelAlgorithm=] set to [=processorCancel=] with [=this=] as parameter, and [=ReadableStream/set up/highWatermark=] set to 0.

The <dfn>processorPull</dfn> algorithm is given a |processor| as input. It is defined by the following steps:
1. Increment the value of the |processor|.`[[numPendingReads]]` by 1.
2. [=Queue a task=] to run the [=maybeReadFrame=] algorithm with |processor| as parameter.
3. Return  [=a promise resolved with=] undefined.

The <dfn>maybeReadFrame</dfn> algorithm is given a |processor| as input. It is defined by the following steps:
1. If |processor|.`[[queue]]` is {{Queue/empty}}, abort these steps.
2. If |processor|.`[[numPendingReads]]` equals zero, abort these steps.
3. {{Queue/dequeue}} a frame from |processor|.`[[queue]]` and [=ReadableStream/Enqueue=] it in |processor|.{{MediaStreamTrackProcessor/readable}}.
4. Decrement |processor|.`[[numPendingReads]]` by 1.
5. Go to step 1.

The <dfn>processorCancel</dfn> algorithm is given a |processor| as input.
It is defined by running the following steps:
1. Run the [=processorClose=] algorithm with |processor| as parameter.
3. Return  [=a promise resolved with=] undefined.

The <dfn>processorClose</dfn> algorithm is given a |processor| as input.
It is defined by running the following steps:
1. If |processor|.`[[isClosed]]` is true, abort these steps.
2. Disconnect |processor| from |processor|.`[[track]]`. The mechanism to do this is UA specific and the result is that |processor| is no longer a sink of |processor|.`[[track]]`.
3. {{ReadableStreamDefaultControllerClose|Close}} |processor|.{{MediaStreamTrackProcessor/readable}}.{{ReadableStreamControllerSlot|[[controller]]}}.
4. {{List/Empty}} |processor|.`[[queue]]`.
5. Set |processor|.`[[isClosed]]` to true.

</dd>
</dl>

### Handling interaction with the track ### {#processor-handling-interaction-with-track}
When the `[[track]]` of a {{MediaStreamTrackProcessor}} |processor| delivers a
frame to |processor|, the UA MUST execute the [=handleNewFrame=] algorithm
with |processor| as parameter.

The <dfn>handleNewFrame</dfn> algorithm is given a |processor| as input.
It is defined by running the following steps:
1. If |processor|.`[[maxBufferSize]]` has a value and |processor|.`[[queue]]` has |processor|.`[[maxBufferSize]]` elements, {{Queue/dequeue}} an item from |processor|.`[[queue]]`.
2. {{Queue/enqueue}} the new frame in |processor|.`[[queue]]`.
3. [=Queue a task=] to run the [=maybeReadFrame=] algorithm with |processor| as parameter.

At any time, the UA MAY {{List/remove}} any frame from |processor|.`[[queue]]`.
The UA may decide to remove frames from |processor|.`[[queue]]`, for example,
to prevent resource exhaustion or to improve performance in certain situations.
</dd>

<p class="note">
The application may detect that frames have been dropped by noticing that there
is a gap in the timestamps of the frames.
</p>
</dl>

When the `[[track]]` of a {{MediaStreamTrackProcessor}} |processor|
{{TrackEnded|ends}}, the [=processorClose=] algorithm must be
executed with |processor| as parameter.

## MediaStreamTrackGenerator ## {#generator}
A {{MediaStreamTrackGenerator}} allows the creation of a {{WritableStream}}
that acts as a {{MediaStreamTrack}} source in the
<a href="https://www.w3.org/TR/mediacapture-streams/#the-model-sources-sinks-constraints-and-settings">
MediaStream model</a>. Since the model does not expose sources directly
but through the tracks connected to it, a {{MediaStreamTrackGenerator}}
is also a track connected to its {{WritableStream}} source. Further tracks
connected to the same {{WritableStream}} can be created using the
{{MediaStreamTrack/clone}} method. The {{WritableStream}} source is
exposed as the {{MediaStreamTrackGenerator/writable}} field of
{{MediaStreamTrackGenerator}}.

Similarly to {{MediaStreamTrackProcessor}}, the {{WritableStream}} of
an audio {{MediaStreamTrackGenerator}} accepts {{AudioData}} objects,
and a video {{MediaStreamTrackGenerator}} accepts {{VideoFrame}} objects.
When a {{VideoFrame}} or {{AudioData}} object is written to
{{MediaStreamTrackGenerator/writable}},
the frame's `close()` method is automatically invoked, so that its internal
resources are no longer accessible from JavaScript.

### Interface definition ### {#generator-interface}
<pre class="idl">
interface MediaStreamTrackGenerator : MediaStreamTrack {
    constructor(MediaStreamTrackGeneratorInit init);
    attribute WritableStream writable;  // VideoFrame or AudioFrame
};

dictionary MediaStreamTrackGeneratorInit {
  required DOMString kind;
};
</pre>

### Constructor ### {#generator-constructor}
<dfn constructor for=MediaStreamTrackGenerator title="MediaStreamTrackGenerator(init)">
  MediaStreamTrackGenerator(init)
</dfn>
1. If |init|.{{MediaStreamTrackGeneratorInit/kind}} is not `"audio"` or `"video"`,
    throw a {{TypeError}}.
2. Let |g| be a new {{MediaStreamTrackGenerator}} object.
3. Initialize the {{MediaStreamTrack/kind}} field of |g| (inherited from {{MediaStreamTrack}})
    with |init|.{{MediaStreamTrackGeneratorInit/kind}}.
4. Return |g|.

### Attributes ### {#generator-attributes}
<dl>
<dt><dfn attribute for=MediaStreamTrackGenerator>writable</dfn></dt>
<dd>Allows writing media frames to the {{MediaStreamTrackGenerator}}, which is
itself a {{MediaStreamTrack}}. When this attribute is access for the first time,
it MUST be initialized with the following steps:
1. Initialize [=this=].{{MediaStreamTrackGenerator/writable}} to be a new {{WritableStream}}.
2. <a dfn for="WritableStream">Set up</a> [=this=].{{MediaStreamTrackGenerator/writable}}, with its [=WritableStream/set up/writeAlgorithm=] set to [=writeFrame=] with [=this=] as parameter, [=WritableStream/set up/closeAlgorithm=] set to [=closeWritable=] with [=this=] as parameter, and [=WritableStream/set up/abortAlgorithm=] set to [=closeWritable=] with [=this=] as parameter.

The <dfn>writeFrame</dfn> algorithm is given a |generator| and a |frame| as input. It is defined by running the following steps:
1. If |generator|.{{MediaStreamTrack/kind}} equals `video` and |frame| is not a {{VideoFrame}} object, return [=a promise rejected with=] a {{TypeError}}.
2. If |generator|.{{MediaStreamTrack/kind}} equals `audio` and |frame| is not an {{AudioData}} object, return [=a promise rejected with=] a {{TypeError}}.
3. Send the media data backing |frame| to all live tracks connected to |generator|, possibly including |generator| itself.
4. Invoke the `close` method of |frame|.
5. Return  [=a promise resolved with=] undefined.

The <dfn>closeWritable</dfn> algorithm is given a |generator| as input.
It is defined by running the following steps.
1. {{StopSource|Stop}} the generator as a MediaStreamTrack source.

<p class="note">
A consecuence of this step is that all tracks connected to
|generator|.{{MediaStreamTrackGenerator/writable}} will be ended.
</p>
2. Return [=a promise resolved with=] undefined.


</dd>
</dl>

### Specialization of MediaStreamTrack behavior ### {#generator-as-track}
A {{MediaStreamTrackGenerator}} is a {{MediaStreamTrack}}. This section adds
clarifications on how a {{MediaStreamTrackGenerator}} behaves as a
{{MediaStreamTrack}}.

#### clone #### {#generator-clone}
The {{MediaStreamTrack/clone}} method on a {{MediaStreamTrackGenerator}}
returns a new {{MediaStreamTrack}} object whose source is the
same as the one for the {{MediaStreamTrackGenerator}} being cloned.
This source is the {{MediaStreamTrackGenerator/writable}} field of
the {{MediaStreamTrackGenerator}}.

#### stop #### {#generator-stop}
The {{MediaStreamTrack/stop}} method on a {{MediaStreamTrackGenerator}} stops
the track. When the last track connected to
the {{MediaStreamTrackGenerator/writable}} of a {{MediaStreamTrackGenerator}}
ends, its {{MediaStreamTrackGenerator/writable}} is [=WritableStream/closing|closed=].

#### Constrainable properties #### {#generator-constrainable-properties}

The following constrainable properties are defined for video
{{MediaStreamTrackGenerator}}s and any {{MediaStreamTrack}}s sourced from
a {{MediaStreamTrackGenerator}}:
<table>
  <thead>
    <tr>
      <th>
        Property Name
      </th>
      <th>
        Values
      </th>
      <th>
        Notes
      </th>
    </tr>
  </thead>
  <tbody>
    <tr id="def-constraint-width">
      <td data-tests="">
        <dfn>width</dfn>
      </td>
      <td>
        {{ConstrainULong}}
      </td>
      <td>
        As a setting, this is the width, in pixels, of the latest
        frame delivered by the track.
        As a capability, `max` MUST reflect the
        largest width a {{VideoFrame}} may have, and `min`
        MUST reflect the smallest width a {{VideoFrame}} may have.
      </td>
    </tr>
    <tr id="def-constraint-height">
      <td data-tests="">
        <dfn>height</dfn>
      </td>
      <td>
        {{ConstrainULong}}
      </td>
      <td>
        As a setting, this is the height, in pixels, of the latest
        frame delivered by the track.
        As a capability, `max` MUST reflect the largest height
        a {{VideoFrame}} may have, and `min` MUST reflect
        the smallest height a {{VideoFrame}} may have.
      </td>
    </tr>
    <tr id="def-constraint-frameRate">
      <td data-tests="">
        <dfn>frameRate</dfn>
      </td>
      <td>
        {{ConstrainDouble}}
      </td>
      <td>
        As a setting, this is an estimate of the frame rate based on frames
        recently delivered by the track.
        As a capability `min` MUST be zero and
        `max` MUST be the maximum frame rate supported by the system.
      </td>
    </tr>
    <tr id="def-constraint-aspect">
      <td data-tests="">
        <dfn>aspectRatio</dfn>
      </td>
      <td>
        {{ConstrainDouble}}
      </td>
      <td>
        As a setting, this is the aspect ratio of the latest frame
        delivered by the track;
        this is the width in pixels divided by height in pixels as a
        double rounded to the tenth decimal place. As a capability,
        `min` MUST be the
        smallest aspect ratio supported by a {{VideoFrame}}, and `max` MUST be
        the largest aspect ratio supported by a {{VideoFrame}}.
      </td>
    </tr>
    <tr id="def-constraint-resizeMode">
      <td data-tests="">
        <dfn>resizeMode</dfn>
      </td>
      <td>
        {{ConstrainDOMString}}
      </td>
      <td>
        As a setting, this string should be one of the members of
        {{VideoResizeModeEnum}}. The value "{{VideoResizeModeEnum/none}}"
        means that the frames output by the MediaStreamTrack are unmodified
        versions of the frames written to the
        {{MediaStreamTrackGenerator/writable}} backing
        the track, regardless of any constraints.
        The value "{{VideoResizeModeEnum/crop-and-scale}}" means
        that the frames output by the MediaStreamTrack may be cropped and/or
        downscaled versions
        of the source frames, based on the values of the width, height and
        aspectRatio constraints of the track.
        As a capability, the values "{{VideoResizeModeEnum/none}}" and
        "{{VideoResizeModeEnum/crop-and-scale}}" both MUST be present.
      </td>
    </tr>
  </tbody>
</table>

The {{MediaStreamTrack/applyConstraints}} method applied to a video {{MediaStreamTrack}}
sourced from a {{MediaStreamTrackGenerator}} supports the properties defined above.
It can be used, for example, to resize frames or adjust the frame rate of the track.
Note that these constraints have no effect on the {{VideoFrame}} objects
written to the {{MediaStreamTrackGenerator/writable}} of a {{MediaStreamTrackGenerator}},
just on the output of the track on which the constraints have been applied.

The following constrainable properties are defined for audio {{MediaStreamTrack}}s
sourced from a {{MediaStreamTrackGenerator}}, but in an informational capacity
only available via {{MediaStreamTrack/getSettings}}. It is not possible to
reconfigure an audio track sourced by a {{MediaStreamTrackGenerator}} using
{{MediaStreamTrack/applyConstraints}} in the same way that it is possble to,
for example, resize the frames of a video track.
{{MediaStreamTrack/getCapabilities}} MUST return an empty object.
<table>
  <thead>
    <tr>
      <th>
        Property Name
      </th>
      <th>
        Values
      </th>
      <th>
        Notes
      </th>
    </tr>
  </thead>
  <tbody>
    <tr id="def-constraint-sampleRate">
      <td data-tests="">
        <dfn>sampleRate</dfn>
      </td>
      <td>
        {{ConstrainDouble}}
      </td>
      <td>
        As a setting, this is the sample rate, in samples per second, of the
        latest {{AudioData}} delivered by the track.
      </td>
    </tr>
    <tr id="def-constraint-channelCount">
      <td data-tests="">
        <dfn>channelCount</dfn>
      </td>
      <td>
        {{ConstrainULong}}
      </td>
      <td>
        As a setting, this is the number of independent audio channels of the
        latest {{AudioData}} delivered by the track.
      </td>
    </tr>
    <tr id="def-constraint-sampleSize">
      <td data-tests="">
        <dfn>sampleSize</dfn>
      </td>
      <td>
        {{ConstrainULong}}
      </td>
      <td>
        As a setting, this is the linear sample size of the latest {{AudioData}}
        delivered by the track.
      </td>
    </tr>
  </tbody>
</table>

#### Events and attributes #### {#generator-events-attributes}
Events and attributes work the same as for any {{MediaStreamTrack}}.
It is relevant to note that if the {{MediaStreamTrackGenerator/writable}}
stream of a {{MediaStreamTrackGenerator}} is closed, all the live
tracks connected to it, possibly including the {{MediaStreamTrackGenerator}}
itself, are ended and the `ended` event is fired on them.

# Examples # {#examples}
Consider a face recognition function `detectFace(videoFrame)` that returns a face position
(in some format), and a manipulation function `blurBackground(videoFrame, facePosition)` that
returns a new VideoFrame similar to the given `videoFrame`, but with the non-face parts blurred.

<pre class="example">
let stream = await getUserMedia({video:true});
let videoTrack = stream.getVideoTracks()[0];
let trackProcessor = new MediaStreamTrackProcessor({track: videoTrack});
let trackGenerator = new MediaStreamTrackGenerator({kind: 'video'});
let transformer = new TransformStream({
   async transform(videoFrame, controller) {
      let facePosition = await detectFace(videoFrame);
      let newFrame = blurBackground(videoFrame, facePosition);
      videoFrame.close();
      controller.enqueue(newFrame);
  }
});

// After this, trackGenerator can be assigned to any sink such as a
// peer connection, or media element.
trackProcessor.readable
    .pipeThrough(transformer)
    .pipeTo(trackGenerator.writable);
</pre>

# Security and Privacy considerations # {#security-considerations}

This API defines a {{MediaStreamTrack}} source and a {{MediaStreamTrack}} sink.
The security and privacy of the source ({{MediaStreamTrackGenerator}}) relies
on the same-origin policy. That is, the data {{MediaStreamTrackGenerator}} can
make available in the form of a {{MediaStreamTrack}} must be visible to
the document before a {{VideoFrame}} or {{AudioData}} object can be constructed
and pushed into the {{MediaStreamTrackGenerator}}. Any attempt to create
{{VideoFrame}} or {{AudioData}} objects using cross-origin data will fail.
Therefore, {{MediaStreamTrackGenerator}} does not introduce any new
fingerprinting surface.

The {{MediaStreamTrack}} sink introduced by this API ({{MediaStreamTrackProcessor}})
exposes {{MediaStreamTrack}} the same data that is exposed by other
{{MediaStreamTrack}} sinks such as WebRTC peer connections, Web Audio
{{MediaStreamAudioSourceNode}} and media elements. The security and privacy
of {{MediaStreamTrackProcessor}} relies on the security and privacy of the
{{MediaStreamTrack}} sources of the tracks to which {{MediaStreamTrackProcessor}}
is connected. For example, camera, microphone and screen-capture tracks
rely on explicit use authorization via permission dialogs (see
[[MEDIACAPTURE-STREAMS]] and [[MEDIACAPTURE-SCREEN-SHARE]]),
while element capture and {{MediaStreamTrackGenerator}}
rely on the same-origin policy.
A potential issue with {{MediaStreamTrackProcessor}} is resource exhaustion.
For example, a site might hold on to too many open {{VideoFrame}} objects
and deplete a system-wide pool of GPU-memory-backed frames. UAs can
mitigate this risk by limiting the number of pool-backed frames a site can
hold. This can be achieved by reducing the maximum number of buffered frames
and by refusing to deliver more frames to {{MediaStreamTrackProcessor/readable}}
once the budget limit is reached. Accidental exhaustion is also mitigated by
automatic closing of {{VideoFrame}} and {{AudioData}} objects once they
are written to a {{MediaStreamTrackGenerator}}.
